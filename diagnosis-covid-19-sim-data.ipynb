{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a2c4d04-fb40-417f-9314-5f5f9fbfc934",
   "metadata": {},
   "source": [
    "## Build a contact network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb5bf8-2f4a-4500-997f-6412fcc63150",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99dfcc92-af01-4ff8-a1b6-4a969b5ca943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://lp-prod-resources.s3.amazonaws.com/628/66549/2021-06-25-19-30-14/PeopleLocations.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4d49c0e-bbe2-4101-841f-5e797258004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import datetime as dt\n",
    "from sklearn import preprocessing\n",
    "from geopy.distance import geodesic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942af55c-95a0-46e7-b8a7-a60cb4c68bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csv to pd.DataFrame\n",
    "df = pd.read_csv(\"PeopleLocations.csv\", sep = \";\", \n",
    "                dtype={'id': np.str_, 'Lat': np.float32, 'Lon': np.float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c2c5eb-01d2-4e22-a0e7-7c718353731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert type to category\n",
    "df[\"Covid19\"] = df[\"Covid19\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec13d5e3-beb7-4f2f-b914-f9a9b051a223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           object\n",
       "Lat         float32\n",
       "Lon         float32\n",
       "Date         object\n",
       "Time         object\n",
       "Covid19    category\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dtypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fca0548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the first column \n",
    "df.rename(columns={\"ID\": \"IDcol\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eecb840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the header of the first colum of dataframe df in variable \"IDcol\"\n",
    "IDcol = df.columns[0]\n",
    "\n",
    "# determine a list of different individuals for which there is at least one record in the csv file\n",
    "uniquepart = df[IDcol].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "628384f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDcol</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lon</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Covid19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Person1</td>\n",
       "      <td>60.185390</td>\n",
       "      <td>25.009689</td>\n",
       "      <td>09-06-2021</td>\n",
       "      <td>13:52:09</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Person2</td>\n",
       "      <td>60.185387</td>\n",
       "      <td>25.009678</td>\n",
       "      <td>09-06-2021</td>\n",
       "      <td>13:52:09</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Person3</td>\n",
       "      <td>60.185390</td>\n",
       "      <td>25.009695</td>\n",
       "      <td>09-06-2021</td>\n",
       "      <td>13:52:09</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Person4</td>\n",
       "      <td>60.185390</td>\n",
       "      <td>25.009689</td>\n",
       "      <td>09-06-2021</td>\n",
       "      <td>13:52:09</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Person5</td>\n",
       "      <td>60.185387</td>\n",
       "      <td>25.009672</td>\n",
       "      <td>09-06-2021</td>\n",
       "      <td>13:52:09</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Person6</td>\n",
       "      <td>60.185394</td>\n",
       "      <td>25.009706</td>\n",
       "      <td>09-06-2021</td>\n",
       "      <td>13:52:09</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Person7</td>\n",
       "      <td>60.185383</td>\n",
       "      <td>25.009668</td>\n",
       "      <td>09-06-2021</td>\n",
       "      <td>13:52:09</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Person8</td>\n",
       "      <td>60.185387</td>\n",
       "      <td>25.009686</td>\n",
       "      <td>09-06-2021</td>\n",
       "      <td>13:52:09</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Person9</td>\n",
       "      <td>60.185379</td>\n",
       "      <td>25.009634</td>\n",
       "      <td>09-06-2021</td>\n",
       "      <td>13:52:09</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Person10</td>\n",
       "      <td>60.185387</td>\n",
       "      <td>25.009678</td>\n",
       "      <td>09-06-2021</td>\n",
       "      <td>13:52:09</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      IDcol        Lat        Lon        Date      Time Covid19\n",
       "0   Person1  60.185390  25.009689  09-06-2021  13:52:09       n\n",
       "1   Person2  60.185387  25.009678  09-06-2021  13:52:09       n\n",
       "2   Person3  60.185390  25.009695  09-06-2021  13:52:09       n\n",
       "3   Person4  60.185390  25.009689  09-06-2021  13:52:09       y\n",
       "4   Person5  60.185387  25.009672  09-06-2021  13:52:09       n\n",
       "5   Person6  60.185394  25.009706  09-06-2021  13:52:09       n\n",
       "6   Person7  60.185383  25.009668  09-06-2021  13:52:09       y\n",
       "7   Person8  60.185387  25.009686  09-06-2021  13:52:09       n\n",
       "8   Person9  60.185379  25.009634  09-06-2021  13:52:09       n\n",
       "9  Person10  60.185387  25.009678  09-06-2021  13:52:09       n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a bit of the data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22ee480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrfeatures=6\n",
    "\n",
    "# store the first colum of dataframe df in variable \"IDcol\"\n",
    "\n",
    "IDcol = df.columns[0]\n",
    "\n",
    "# determine a list of different individuals for which there is at least one record in the csv file\n",
    "\n",
    "uniquepart = df[IDcol].unique()       \n",
    "\n",
    "# count the number of different individuals. this will be the number of nodes in the contace network\n",
    "\n",
    "nrnodes = len(uniquepart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08efc6a0",
   "metadata": {},
   "source": [
    "To build the contact network we add an edge between nodes representing individuals for which we can find location recording which are closer than 2 meters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98fd7767",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['x1'], dtype='object')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iterfeature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nrfeatures):\n\u001b[1;32m     65\u001b[0m     keytmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39m (iterfeature\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m     dmyvec[iterfeature]\u001b[38;5;241m=\u001b[39m\u001b[43mdmydf_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkeytmp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# set the node attribute \"x\" to the numpy array \"dmyvec\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m G\u001b[38;5;241m.\u001b[39mnodes[nodeidx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dmyvec\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fl-predict-covid-19-MRVSllAs/lib/python3.8/site-packages/pandas/core/indexing.py:1067\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1066\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m-> 1067\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1069\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fl-predict-covid-19-MRVSllAs/lib/python3.8/site-packages/pandas/core/indexing.py:1247\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   1246\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_ellipsis(tup)\n\u001b[0;32m-> 1247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fl-predict-covid-19-MRVSllAs/lib/python3.8/site-packages/pandas/core/indexing.py:991\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    989\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m section\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# This is an elided recursive call to iloc/loc\u001b[39;00m\n\u001b[0;32m--> 991\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot applicable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fl-predict-covid-19-MRVSllAs/lib/python3.8/site-packages/pandas/core/indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1070\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1072\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fl-predict-covid-19-MRVSllAs/lib/python3.8/site-packages/pandas/core/indexing.py:1301\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1299\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fl-predict-covid-19-MRVSllAs/lib/python3.8/site-packages/pandas/core/indexing.py:1239\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1239\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1241\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fl-predict-covid-19-MRVSllAs/lib/python3.8/site-packages/pandas/core/indexing.py:1432\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1429\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1430\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1432\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fl-predict-covid-19-MRVSllAs/lib/python3.8/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fl-predict-covid-19-MRVSllAs/lib/python3.8/site-packages/pandas/core/indexes/base.py:6130\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6129\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['x1'], dtype='object')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "# create networkx object `G` by adding nodes for each individual with a record in \"PeopleLocations.csv\"\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "# we use a label encoder used to transfrom values 'y'/'n' for Covid19 infection to values 1 and 0\n",
    "le = preprocessing.LabelEncoder()                \n",
    "le.fit([\"n\", \"y\"])\n",
    "\n",
    "# iterate over individuals represnted by network nodes indexed by nodeidx=0,1,...\n",
    "\n",
    "for nodeidx in range(nrnodes): \n",
    "    \n",
    "    # read in identifier of individual from list `uniquepart` and store in variable \"personid\"\n",
    "    personid = uniquepart[nodeidx]\n",
    "    \n",
    "    # create dataframe \"dmydf\" by selecting all rows from dataframe `df` with attribute `ID` equal to `personid`\n",
    "    dmydf = pd.DataFrame(df.loc[df['IDcol'] == personid].copy())\n",
    "    # create dataframe \"dmydf_features\" by selecting all rows from dataframe `df` with attribute `ID` equal to `personid`\n",
    "    dmydf_features = pd.DataFrame(df.loc[df['IDcol'] == personid].copy())\n",
    "    \n",
    "    # reset index of dataframe dmydf \n",
    "    dmydf.reset_index(drop=True, inplace=True) \n",
    "    # reset index of dataframe dmydf_features \n",
    "    dmydf_features.reset_index(drop=True, inplace=True) \n",
    "    \n",
    "    # read in latitude of first location recording in `dmydf` and store in variable `latitude`\n",
    "    latitude=dmydf.loc[0,['Lat']][0]\n",
    "    \n",
    "    # read in longitude of first location recording in `dmydf` and store in variable `longitude`\n",
    "    longitude=dmydf.loc[0,['Lon']][0]\n",
    "    \n",
    "    # read in Covid19 infection status of first location recording in `dmydf` and store in variable `valtmp`\n",
    "    valtmp=dmydf.loc[0,['Covid19']][0]\n",
    "    \n",
    "    # use le.transform() to map the infection status `valtmp` as `y`->1 and `n`-> 0\n",
    "    infected=le.transform([valtmp])\n",
    "    \n",
    "    # read in the date of the recording and store in variable date_tmp\n",
    "    date_tmp = dt.datetime.strptime(dmydf.loc[0,['Date']][0], '%d-%m-%Y').date() \n",
    "    \n",
    "    # read in the time of the recording and store in variable time_tmp\n",
    "    time_tmp = dt.datetime.strptime(dmydf.loc[0,['Time']][0], '%H:%M:%S').time()\n",
    "    \n",
    "    # combine date and time of location racording using `datetime.combine()\n",
    "    mydatetime = dt.datetime.combine(date_tmp, time_tmp)\n",
    "    \n",
    "    # add a node with index `nodeidx`\n",
    "    G.add_node(nodeidx)\n",
    "    # set the node attribute \"name\" to the string stored in \"personid\"\n",
    "    G.nodes[nodeidx]['name']= personid\n",
    "    # set the node attribute \"coords\" to a numpy array with entries \"latitude\" and \"longitude\"\n",
    "    G.nodes[nodeidx]['coords']= np.array([latitude,longitude])\n",
    "    # set the node attribute \"timestamp\" to the value of \"mydatetime\"\n",
    "    G.nodes[nodeidx]['timestamp'] = mydatetime\n",
    "    # set the node attribute \"y\" equal to 1 if individual has been reported as Covid-19 infected and 0 otherwise\n",
    "    G.nodes[nodeidx]['y'] = infected[0] \n",
    "    # set the node attribute \"w\" to a numpy array of shape (6,) and entries all zero\n",
    "    G.nodes[nodeidx]['w'] = np.zeros(nrfeatures)    \n",
    "    # set the node attribute \"b\" to 0.0\n",
    "    G.nodes[nodeidx]['b'] = 0.0  \n",
    "\n",
    "    # read in the features x1,...,x6 from dataframe \"dmydf_features\" and store in numpy array \"dmyvec\"\n",
    "    dmyvec = np.zeros(nrfeatures)\n",
    "    for iterfeature in range(nrfeatures):\n",
    "        keytmp = \"x%d\"% (iterfeature+1)\n",
    "        dmyvec[iterfeature]=dmydf_features.loc[0,[keytmp]][0]\n",
    "    \n",
    "    # set the node attribute \"x\" to the numpy array \"dmyvec\"\n",
    "    G.nodes[nodeidx]['x'] = dmyvec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67053978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two nested for-loops over node indices 0,1,...,nrnodes-1 \n",
    "# the loop variables are named \"nodeidx1\" and \"nodeidx2\"\n",
    "\n",
    "for nodeidx1 in range(nrnodes): \n",
    "    for nodeidx2 in range(nrnodes): \n",
    "        # test if nodeidx1 is different from nodeidx2\n",
    "        if nodeidx1!=nodeidx2 : \n",
    "            # compute the geodesic distance between individualas \"nodeidx1\" and \"nodeidx2\" in meters \n",
    "            nodedist=geodesic(G.nodes[nodeidx1]['coords'],G.nodes[nodeidx2]['coords']).meters\n",
    "            # if distance is below two meters connect invididuals by and edge. \n",
    "            if  nodedist<2: \n",
    "                G.add_edge(nodeidx1,nodeidx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new graph object \"SubGraph\" using G.subgraph() consisting of nodes 0,1,2,3,4\n",
    "SubGraph = G.subgraph([0,1,2,3,4])\n",
    "\n",
    "# read out node attribute `b`from all nodes in \"SubGraph\" and store in variable \"labels\"\n",
    "labels = nx.get_node_attributes(SubGraph, 'b') \n",
    "\n",
    "# plot \"SubGraph\" using nx.draw_networkx() with \"labels\" as node labels \n",
    "nx.draw_networkx(SubGraph,labels = labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b02c3f",
   "metadata": {},
   "source": [
    "Personalized Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431890d5",
   "metadata": {},
   "source": [
    "his milestone requires you to learn personalized predictors for a Covid-19 infection. To this end you will the combine the gradient descent algorithm for logistic regression with a network averaging method for aggregating local gradients computed for each individual. \n",
    "\n",
    "More formally, we assign each invidiual $i$ a linear classifier with weight vector $\\mathbf{w}^{(i)}=\\big(w^{(i)}_{1},\\ldots,w^{(i)}_{6}\\big)^{T}$ and intercept (bias) term $b^{(i)}$. Given an individual $i$ with features $\\mathbf{x}^{(i)}$ (extracted from an audio recording) we diagnose a Covid-19 infection if $\\mathbf{w}^{T} \\mathbf{x}^{(i)} +b^{(i)} \\geq0$. To learn the weight vector and  intercept term for the node $i$ that belongs to the component $\\mathcal{C}$ of the contact network, we use a sufficient number of gradient descent steps\n",
    "$$ \\mathbf{w}^{(k+1)} = \\mathbf{w}^{(k)} - \\alpha \\mathbf{g}^{(k)} \\mbox{ with } \\mathbf{g}^{(k)}= (1/|\\mathcal{C}|) \\sum_{j \\in \\mathcal{C}} \\big(h\\big(\\big(\\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(j)}\\big) - y^{(j)}\\big) \\mathbf{x}^{(j)} $$ \n",
    "and\n",
    "$$ b^{(k+1)} = b^{(k)} - \\alpha v^{(k)} \\mbox{ with } v^{(k)}= (1/|\\mathcal{C}|) \\sum_{j \\in \\mathcal{C}} \\big(h\\big(\\big(\\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(j)}\\big) - y^{(j)}\\big)  $$. \n",
    "\n",
    "We will estimate the gradients $\\mathbf{g}^{(k)}$ and $v^{(k)}$ using the averaging algorithm that we used in Project 2 for computing the average infection rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82edc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sigmoid function\n",
    "# helps us computer the probability of an individual being infected\n",
    "# with Covid 19\n",
    "\n",
    "def sigmoid(X, theta):\n",
    "    '''\n",
    "    Computes the sigmoid of the linear combination of X and theta.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape (n, m)\n",
    "\n",
    "    theta : numpy array of shape (m,)\n",
    "        The parameters of the logistic regression model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy array of shape (n,)\n",
    "        The sigmoid of the linear combination of X and theta.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    >>> theta = np.array([1, 2, 3])\n",
    "    >>> sigmoid(X, theta)\n",
    "    array([0.99987661, 1.        ])\n",
    "    '''\n",
    "    # compute the linear combination of x and theta\n",
    "    z = np.dot(X, theta[1:]) + theta[0]\n",
    "\n",
    "    # compute the sigmoid of z\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps each node to a dictionary of its neighbors\n",
    "weights_tmp_dic=nx.get_node_attributes(G,'w')\n",
    "\n",
    "# make zeros based on the number of nodes and features\n",
    "weights_tmp = np.zeros((nrnodes,nrfeatures))\n",
    "\n",
    "# maps each node to an intercept value\n",
    "intercept_tmp_dic=nx.get_node_attributes(G,'b')\n",
    "\n",
    "# maps zero based on the number of nodes\n",
    "intercept_tmp = np.zeros(nrnodes)\n",
    "\n",
    "# maps each node to a feature vector\n",
    "features_tmp_dic=nx.get_node_attributes(G,'x')\n",
    "\n",
    "# make zeros based on the number of nodes and features\n",
    "features_tmp = np.zeros((nrnodes,nrfeatures))\n",
    "\n",
    "# maps each node to a label\n",
    "label_tmp_dic=nx.get_node_attributes(G,'y')\n",
    "\n",
    "# maps zero based on the number of nodes\n",
    "label_tmp = np.zeros(nrnodes)\n",
    "\n",
    "# loop over all nodes\n",
    "for iternode in range(nrnodes):\n",
    "      weights_tmp[iternode,:] = weights_tmp_dic[iternode]\n",
    "      intercept_tmp[iternode] = intercept_tmp_dic[iternode]\n",
    "      features_tmp[iternode,:] = features_tmp_dic[iternode]\n",
    "      label_tmp[iternode] = label_tmp_dic[iternode]\n",
    "\n",
    "# set step-size\n",
    "alpha = 1/10\n",
    "\n",
    "# copy weights and intercept to new variables\n",
    "weights_old = weights_tmp.copy() \n",
    "intercept_old = intercept_tmp.copy()\n",
    "gradient_tmp = np.zeros((nrnodes,nrfeatures+1)) # each row hold the gradient for intercept and weights \n",
    "gradient_old = np.zeros((nrnodes,nrfeatures+1)) \n",
    "\n",
    "# 50 iterations\n",
    "nriters=50\n",
    "\n",
    "# create \"Metropolis-Hastings\" weights and store them in numpy array `W_MH`\n",
    "W_MH = np.zeros((nrnodes,nrnodes)) # create array for MH weights and init to all zeroes\n",
    "# iterate over all edges in the contact network G\n",
    "for edge in G.edges(): \n",
    "    node_a = edge[0] # first node of edge\n",
    "    node_b = edge[1] # second node of edge\n",
    "    # set weights W[node_a,node_b] and W[node_b,node_a] to 1/(max(degree(node_a),degree(node_b))+1)\n",
    "    W_MH[node_a,node_b] = 1/(np.max([G.degree(node_a),G.degree(node_b)])+1) \n",
    "\n",
    "    # set weights W[node_a,node_b] and W[node_b,node_a] to 1/(max(degree(node_a),degree(node_b))+1)\n",
    "    W_MH[node_b,node_a] = 1/(np.max([G.degree(node_a),G.degree(node_b)])+1)\n",
    "\n",
    "# loop over all nodes in the contact network G\n",
    "for nodedmy in G.nodes(): \n",
    "# set weights W[nodedmy,nodedmy] to 1 - sum of weights for all neighbors of nodedmy\n",
    "    W_MH[nodedmy,nodedmy] = 1-np.sum(W_MH[nodedmy,:])\n",
    "    \n",
    "# set number of iterations for gradient descent to default value 200\n",
    "nrlogregiters = 10\n",
    "\n",
    "# main loop for the federated learning algorithm \n",
    "# each iteration amounts to network averaging of all local gradients \n",
    "\n",
    "for iterlogreg in range(nrlogregiters):\n",
    "# compute gradients at each node \n",
    "    for iternode in range(nrnodes):\n",
    "# stack weights and intercept into theta\n",
    "        theta = np.hstack((intercept_tmp[iternode],weights_tmp[iternode]))\n",
    "        # compute sgmoid function of predictor value w^T x\n",
    "        hx = sigmoid(features_tmp[iternode], theta)\n",
    "        # calculate error\n",
    "        error = hx - label_tmp[iternode]\n",
    "        # compute gradient for local loss function and store in gradient_tmp\n",
    "        gradient_tmp[iternode,:] = np.hstack((error,error*features_tmp[iternode]))\n",
    "          \n",
    "    \n",
    "    \n",
    "# average gradients using nriters consensus iterations\n",
    "    for iterdmy in range(nriters):\n",
    "        # read in current values of \"Rate\" attributes into numpy array `graphsigold`\n",
    "        gradient_old = gradient_tmp \n",
    "        # update estimate \"gradient_tmp\" by applying W_MH to current estimate\n",
    "        gradient_tmp = np.dot(W_MH, gradient_old)\n",
    "    \n",
    "    # do a gradient descent step for intercept_tmp using step size alpha\n",
    "    intercept_tmp -= alpha*gradient_tmp[:,0]\n",
    "    # do a gradient descent step for weights_tmp using step size alpha\n",
    "    weights_tmp -= alpha*gradient_tmp[:,1:]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# loop over all nodes in the contact network G store the weights in \"weights_tmp\" in the node attribute \"weights\"\n",
    "# store the incepts in \"intercept_tmp\" in the node attribute \"intercep\"\n",
    "\n",
    "for node_i in G.nodes(data=False): \n",
    "    G.node[node_i]['w'] = weights_tmp[node_i]\n",
    "    G.node[node_i]['b'] = intercept_tmp[node_i]\n",
    "  \n",
    "    print(\"weights node %d :\"%node_i,weights_tmp[node_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c25ac8",
   "metadata": {},
   "source": [
    "# summary of the project\n",
    "\n",
    "In this project, we will build a contact network from the location data of the individuals. Then we will use the contact network to learn personalized predictors for a Covid-19 infection. To this end you will the combine the gradient descent algorithm for logistic regression with a network averaging method for aggregating local gradients computed for each individual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830913d3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a6515246a4d677b32bcae4f85f074a2a28abed2fb888ef87af1449dc43f51527"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
